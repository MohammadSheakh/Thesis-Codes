{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from PIL import Image\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.ticker as ticker\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scans found: 0 , Total Headers 112120\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('E:/A__CVPR/Dataset/bbox/Data_Entry_2017_v2020.csv')\n",
    "data = data[data['Patient Age'] < 100]  # Removing invalid data points\n",
    "data_image_paths = {os.path.basename(x): x for x in glob(os.path.join('D:/New CX/CXR8/images', '**', '*.png'))}\n",
    "print('Scans found:', len(data_image_paths), ', Total Headers', data.shape[0])\n",
    "\n",
    "data['path'] = data['Image Index'].map(data_image_paths.get)\n",
    "data['Patient Age'] = data['Patient Age'].map(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data and create labels\n",
    "data['Finding Labels'] = data['Finding Labels'].map(lambda x: x.replace('No Finding', ''))\n",
    "from itertools import chain\n",
    "all_labels = np.unique(list(chain(*data['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n",
    "all_labels = [x for x in all_labels if len(x) > 0]\n",
    "\n",
    "for c_label in all_labels:\n",
    "    if len(c_label) > 1:  # leave out empty labels\n",
    "        data[c_label] = data['Finding Labels'].map(lambda finding: 1.0 if c_label in finding else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Labels: [('Atelectasis', 11559), ('Cardiomegaly', 2776), ('Consolidation', 4667), ('Edema', 2303), ('Effusion', 13317), ('Emphysema', 2516), ('Fibrosis', 1686), ('Infiltration', 19894), ('Mass', 5782), ('Nodule', 6331), ('Pleural_Thickening', 3385), ('Pneumonia', 1431), ('Pneumothorax', 5302)]\n"
     ]
    }
   ],
   "source": [
    "# Filter out cases with at least 1000 samples\n",
    "MIN_CASES = 1000\n",
    "all_labels = [c_label for c_label in all_labels if data[c_label].sum() > MIN_CASES]\n",
    "print('Clean Labels:', [(c_label, int(data[c_label].sum())) for c_label in all_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to have a balanced dataset\n",
    "sample_weights = data['Finding Labels'].map(lambda x: len(x.split('|')) if len(x) > 0 else 0).values + 4e-2\n",
    "sample_weights /= sample_weights.sum()\n",
    "data = data.sample(40000, weights=sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectors for disease labels\n",
    "data['disease_vec'] = data.apply(lambda x: [x[all_labels].values], 1).map(lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation, and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(data, test_size=0.20, random_state=2018, stratify=data['Finding Labels'].map(lambda x: x[:4]))\n",
    "train_df, valid_df = train_test_split(train_df, test_size=0.10, random_state=2018, stratify=train_df['Finding Labels'].map(lambda x: x[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 28800\n",
      "Number of validation samples: 3200\n",
      "Found 0 validated image filenames.\n",
      "Found 0 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the custom data generator for loading images from DataFrame\n",
    "def flow_from_dataframe(img_data_gen, in_df, path_col, y_col, **dflow_args):\n",
    "    df_gen = img_data_gen.flow_from_dataframe(\n",
    "        dataframe=in_df,\n",
    "        directory=None,  # Since full paths are provided in the 'path' column\n",
    "        x_col=path_col,\n",
    "        y_col=y_col,\n",
    "        class_mode='raw',  # Use 'raw' for multi-label classification\n",
    "        **dflow_args\n",
    "    )\n",
    "    return df_gen\n",
    "\n",
    "# Set up ImageDataGenerator with the appropriate preprocessing function for DenseNet\n",
    "core_idg_dense = ImageDataGenerator(preprocessing_function=tf.keras.applications.densenet.preprocess_input)\n",
    "\n",
    "# Set the image size\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "# Ensure that all values in the 'path' column are strings and drop rows with NaN or invalid paths\n",
    "train_df['path'] = train_df['path'].astype(str)\n",
    "valid_df['path'] = valid_df['path'].astype(str)\n",
    "\n",
    "# Drop rows where paths are NaN or empty\n",
    "train_df = train_df[train_df['path'].notna()]\n",
    "valid_df = valid_df[valid_df['path'].notna()]\n",
    "\n",
    "# Check for any issues with the paths (optional debugging step)\n",
    "print(f\"Number of training samples: {train_df.shape[0]}\")\n",
    "print(f\"Number of validation samples: {valid_df.shape[0]}\")\n",
    "\n",
    "# Now create the generators again\n",
    "train_gen = flow_from_dataframe(core_idg_dense, train_df, \n",
    "                                path_col='path', y_col='disease_vec', \n",
    "                                target_size=IMG_SIZE, color_mode='rgb', \n",
    "                                batch_size=16)\n",
    "\n",
    "valid_gen = flow_from_dataframe(core_idg_dense, valid_df, \n",
    "                                path_col='path', y_col='disease_vec', \n",
    "                                target_size=IMG_SIZE, color_mode='rgb', \n",
    "                                batch_size=32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DenseNet model\n",
    "img_in = Input(shape=(224, 224, 3))\n",
    "base_model = DenseNet121(include_top=False, weights='imagenet', input_tensor=img_in, pooling='avg')\n",
    "x = base_model.output\n",
    "predictions = Dense(len(all_labels), activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "model = Model(inputs=img_in, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training generator samples: 0\n",
      "Validation generator samples: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training generator samples: {train_gen.samples}\")\n",
    "print(f\"Validation generator samples: {valid_gen.samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Asked to retrieve element 0, but the Sequence has length 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Application Installed\\anacondaa\\envs\\sheakh310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\Application Installed\\anacondaa\\envs\\sheakh310\\lib\\site-packages\\keras\\preprocessing\\image.py:103\u001b[0m, in \u001b[0;36mIterator.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to retrieve element \u001b[39m\u001b[38;5;132;01m{idx}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut the Sequence \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas length \u001b[39m\u001b[38;5;132;01m{length}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx\u001b[38;5;241m=\u001b[39midx, length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[0;32m    107\u001b[0m         )\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m         np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_batches_seen)\n",
      "\u001b[1;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_gen, steps_per_epoch=100, validation_data=valid_gen, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheakh310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
