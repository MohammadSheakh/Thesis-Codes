{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/mohamedsameh0410/lung-cancer-detection-with-cnn-efficientnetb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import system libs\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "# import data handling tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# import Deep learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "\n",
    "# ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "def loading_the_data(data_dir):\n",
    "    # Generate data paths with labels\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "\n",
    "    # Get folder names\n",
    "    folds = os.listdir(data_dir)\n",
    "\n",
    "    for fold in folds:\n",
    "        foldpath = os.path.join(data_dir, fold)\n",
    "        filelist = os.listdir(foldpath)\n",
    "        for file in filelist:\n",
    "            fpath = os.path.join(foldpath, file)\n",
    "            \n",
    "            filepaths.append(fpath)\n",
    "            labels.append(fold)\n",
    "\n",
    "    # Concatenate data paths with labels into one DataFrame\n",
    "    Fseries = pd.Series(filepaths, name='filepaths')\n",
    "    Lseries = pd.Series(labels, name='labels')\n",
    "\n",
    "    df = pd.concat([Fseries, Lseries], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# change label names to its original names\n",
    "def change_label_names(df, column_name):\n",
    "    index = {'lung_aca': 'Lung_adenocarcinoma', 'lung_n': 'Lung_benign_tissue', 'lung_scc': 'Lung squamous_cell_carcinoma'}\n",
    "\n",
    "\n",
    "    df[column_name] = df[column_name].replace(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "data_dir = 'E:/A__CVPR/Dataset/lung_colon_image_set/lung_image_sets'\n",
    "df = loading_the_data(data_dir)\n",
    "\n",
    "change_label_names(df, 'labels')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "# first we will check if the training data is balanced or not\n",
    "data_balance = df.labels.value_counts()\n",
    "\n",
    "\n",
    "def custom_autopct(pct):\n",
    "    total = sum(data_balance)\n",
    "    val = int(round(pct*total/100.0))\n",
    "    return \"{:.1f}%\\n({:d})\".format(pct, val)\n",
    "\n",
    "\n",
    "# pie chart for data balance\n",
    "plt.pie(data_balance, labels = data_balance.index, autopct=custom_autopct, colors = [\"#2092E6\",\"#6D8CE6\",\"#20D0E6\"])\n",
    "plt.title(\"Training data balance\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# It is balanced, now we will split our data to train, val and test set\n",
    "# data --> 80% train data && 20% (test, val)\n",
    "train_df, ts_df = train_test_split(df, train_size = 0.8, shuffle = True, random_state = 42)\n",
    "\n",
    "# test data --> 10% train data && 10% (test, val)\n",
    "valid_df, test_df = train_test_split(ts_df, train_size = 0.5, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create image data generator\n",
    "\n",
    "# in this step we will convert the whoole data to numpy arrays\n",
    "\n",
    "# crobed image size\n",
    "batch_size = 32\n",
    "img_size = (224, 224)\n",
    "\n",
    "tr_gen = ImageDataGenerator(rescale=1. / 255)\n",
    "ts_gen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n",
    "\n",
    "valid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n",
    "\n",
    "test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                    color_mode= 'rgb', shuffle= False, batch_size= batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample from train data\n",
    "g_dict = train_gen.class_indices      # defines dictionary {'class': index}\n",
    "classes = list(g_dict.keys())       # defines list of dictionary's kays (classes), classes names : string\n",
    "images, labels = next(train_gen)      # get a batch size samples from the generator\n",
    "\n",
    "# ploting the patch size samples\n",
    "plt.figure(figsize= (20, 20))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    plt.subplot(6, 6, i + 1)\n",
    "    image = images[i]\n",
    "    plt.imshow(image)\n",
    "    index = np.argmax(labels[i])  # get image index\n",
    "    class_name = classes[index]   # get class of image\n",
    "    plt.title(class_name, color= 'black', fontsize= 16)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create needed functions\n",
    "# Displaying the model performance\n",
    "def model_performance(history, Epochs):\n",
    "    # Define needed variables\n",
    "    tr_acc = history.history['accuracy']\n",
    "    tr_loss = history.history['loss']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    Epochs = [i+1 for i in range(len(tr_acc))]\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize= (20, 8))\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\n",
    "    plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\n",
    "    plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Evaluate the model\n",
    "def model_evaluation(model):\n",
    "    train_score = model.evaluate(train_gen, verbose= 1)\n",
    "    valid_score = model.evaluate(valid_gen, verbose= 1)\n",
    "    test_score = model.evaluate(test_gen, verbose= 1)\n",
    "    \n",
    "    print(\"Train Loss: \", train_score[0])\n",
    "    print(\"Train Accuracy: \", train_score[1])\n",
    "    print('-' * 20)\n",
    "    print(\"Validation Loss: \", valid_score[0])\n",
    "    print(\"Validation Accuracy: \", valid_score[1])\n",
    "    print('-' * 20)\n",
    "    print(\"Test Loss: \", test_score[0])\n",
    "    print(\"Test Accuracy: \", test_score[1])\n",
    "    \n",
    "\n",
    "# Get Predictions\n",
    "def get_pred(model, test_gen):\n",
    "    \n",
    "    preds = model.predict(test_gen)\n",
    "    y_pred = np.argmax(preds, axis = 1)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "def plot_confusion_matrix(test_gen, y_pred):\n",
    "    \n",
    "    g_dict = test_gen.class_indices\n",
    "    classes = list(g_dict.keys())\n",
    "    \n",
    "    # Display the confusion matrix\n",
    "    cm = confusion_matrix(test_gen.classes, y_pred)\n",
    "\n",
    "    plt.figure(figsize= (10, 10))\n",
    "    plt.imshow(cm, interpolation= 'nearest', cmap= plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation= 45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# Defining a convolutional NN block for a sequential CNN model\n",
    "def conv_block(filters, act='relu'):\n",
    "    \n",
    "    block = Sequential()\n",
    "    block.add(Conv2D(filters, 3, activation=act, padding='same'))\n",
    "    block.add(Conv2D(filters, 3, activation=act, padding='same'))\n",
    "    block.add(BatchNormalization())\n",
    "    block.add(MaxPooling2D())\n",
    "    \n",
    "    return block\n",
    "\n",
    "\n",
    "# Defining a dense NN block for a sequential CNN model\n",
    "def dense_block(units, dropout_rate, act='relu'):\n",
    "    \n",
    "    block = Sequential()\n",
    "    block.add(Dense(units, activation=act))\n",
    "    block.add(BatchNormalization())\n",
    "    block.add(Dropout(dropout_rate))\n",
    "    \n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Structure\n",
    "# CNN model\n",
    "# create Model structure\n",
    "img_size = (224, 224)\n",
    "channels = 3\n",
    "img_shape = (img_size[0], img_size[1], channels)\n",
    "\n",
    "class_counts = len(list(train_gen.class_indices.keys()))     # to define number of classes in dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "cnn_model = Sequential()\n",
    "\n",
    "# first conv block\n",
    "cnn_model.add(Conv2D(filters=16, kernel_size=(3,3), padding=\"same\", activation=\"relu\", input_shape= img_shape))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(MaxPooling2D())\n",
    "\n",
    "# second conv block\n",
    "cnn_model.add(conv_block(32))\n",
    "\n",
    "# third conv block\n",
    "cnn_model.add(conv_block(64))\n",
    "\n",
    "# fourth conv bolck\n",
    "cnn_model.add(conv_block(128))\n",
    "\n",
    "# fifth conv block\n",
    "cnn_model.add(conv_block(256))\n",
    "\n",
    "# flatten layer\n",
    "cnn_model.add(Flatten())\n",
    "\n",
    "# first dense block\n",
    "cnn_model.add(dense_block(128, 0.5))\n",
    "\n",
    "# second dense block\n",
    "cnn_model.add(dense_block(64, 0.3))\n",
    "\n",
    "# third dense block\n",
    "cnn_model.add(dense_block(32, 0.2))\n",
    "\n",
    "# output layer\n",
    "cnn_model.add(Dense(class_counts, activation = \"softmax\"))\n",
    "cnn_model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
    "\n",
    "cnn_model.summary()\n",
    "\n",
    "\n",
    "Model: \"sequential\"\n",
    "# train the model\n",
    "epochs = 20   # number of all epochs in training\n",
    "\n",
    "history = cnn_model.fit(train_gen, epochs= epochs, verbose= 1, validation_data= valid_gen, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "model_evaluation(cnn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# get predictions\n",
    "y_pred = get_pred(cnn_model, test_gen)\n",
    "\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(test_gen, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the pre-trained model (EfficientNetB3)\n",
    "base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape = img_shape, pooling= None)\n",
    "\n",
    "# fine-tune EfficientNetB3 (Adding some custom layers on top)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = dense_block(128, 0.5)(x)\n",
    "x = dense_block(32, 0.2)(x)\n",
    "predictions = Dense(class_counts, activation = \"softmax\")(x)    # output layer with softmax activation\n",
    "\n",
    "# the model\n",
    "EfficientNetB3_model = Model(inputs = base_model.input, outputs = predictions)\n",
    "\n",
    "EfficientNetB3_model.compile(optimizer=Adamax(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "EfficientNetB3_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "epochs = 20   # number of all epochs in training\n",
    "\n",
    "EfficientNetB3_history = EfficientNetB3_model.fit(train_gen, epochs= epochs, verbose= 1, validation_data= valid_gen, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance(EfficientNetB3_history, epochs)\n",
    "\n",
    "\n",
    "# Model evaluation\n",
    "model_evaluation(EfficientNetB3_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# get predictions\n",
    "y_pred = get_pred(EfficientNetB3_model, test_gen)\n",
    "\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(test_gen, y_pred)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheakh310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
