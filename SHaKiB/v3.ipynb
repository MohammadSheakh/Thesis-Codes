{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from glob import glob\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scans found: 112120 , Total Headers 112120\n",
      "All Labels (14): ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax']\n",
      "Clean Labels (13) [('Atelectasis', 11559), ('Cardiomegaly', 2776), ('Consolidation', 4667), ('Edema', 2303), ('Effusion', 13317), ('Emphysema', 2516), ('Fibrosis', 1686), ('Infiltration', 19894), ('Mass', 5782), ('Nodule', 6331), ('Pleural_Thickening', 3385), ('Pneumonia', 1431), ('Pneumothorax', 5302)]\n"
     ]
    }
   ],
   "source": [
    "all_xray_df = pd.read_csv(\"E:/A__CVPR/Dataset/bbox/Data_Entry_2017.csv\")\n",
    "all_image_paths = {os.path.basename(x): x for x in \n",
    "                glob(os.path.join('D:/New CX/CXR8/images', '*.png'))}\n",
    "print('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])\n",
    "all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n",
    "all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n",
    "\n",
    "label_counts = all_xray_df['Finding Labels'].value_counts()[:15]\n",
    "\n",
    "all_xray_df['Finding Labels'] = all_xray_df['Finding Labels'].map(lambda x: x.replace('No Finding', ''))\n",
    "from itertools import chain\n",
    "all_labels = np.unique(list(chain(*all_xray_df['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n",
    "all_labels = [x for x in all_labels if len(x)>0]\n",
    "print('All Labels ({}): {}'.format(len(all_labels), all_labels))\n",
    "for c_label in all_labels:\n",
    "    if len(c_label)>1: # leave out empty labels\n",
    "        all_xray_df[c_label] = all_xray_df['Finding Labels'].map(lambda finding: 1.0 if c_label in finding else 0)\n",
    "\n",
    "# keep at least 1000 cases\n",
    "MIN_CASES = 1000\n",
    "all_labels = [c_label for c_label in all_labels if all_xray_df[c_label].sum()>MIN_CASES]\n",
    "print('Clean Labels ({})'.format(len(all_labels)), \n",
    "    [(c_label,int(all_xray_df[c_label].sum())) for c_label in all_labels])\n",
    "\n",
    "# since the dataset is very unbiased, we can resample it to be a more reasonable collection\n",
    "# weight is 0.1 + number of findings\n",
    "sample_weights = all_xray_df['Finding Labels'].map(lambda x: len(x.split('|')) if len(x)>0 else 0).values + 4e-2\n",
    "sample_weights /= sample_weights.sum()\n",
    "\n",
    "all_xray_df = all_xray_df.sample(40000, weights=sample_weights)\n",
    "\n",
    "label_counts = all_xray_df['Finding Labels'].value_counts()[:15]\n",
    "\n",
    "label_counts = 100*np.mean(all_xray_df[all_labels].values,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xray_df['disease_vec'] = all_xray_df.apply(lambda x: [x[all_labels].values], 1).map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 30000 validation 10000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(all_xray_df, \n",
    "                                test_size = 0.25, \n",
    "                                random_state = 2018,\n",
    "                                stratify = all_xray_df['Finding Labels'].map(lambda x: x[:4]))\n",
    "print('train', train_df.shape[0], 'validation', valid_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df['newLabel'] = valid_df.apply(lambda x: x['Finding Labels'].split('|'), axis=1)\n",
    "train_df['newLabel'] = train_df.apply(lambda x: x['Finding Labels'].split('|'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "core_idg = ImageDataGenerator(samplewise_center=True, \n",
    "                            samplewise_std_normalization=True, \n",
    "                            horizontal_flip = True, \n",
    "                            vertical_flip = False, \n",
    "                            height_shift_range= 0.05, \n",
    "                            width_shift_range=0.1, \n",
    "                            rotation_range=5, \n",
    "                            shear_range = 0.1,\n",
    "                            fill_mode = 'reflect',\n",
    "                            zoom_range=0.15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_weights.best.hdf5\".format('xray_class')\n",
    "\n",
    "checkpoint = ModelCheckpoint('xray_class_weights.best.hdf5',\n",
    "                            monitor='val_loss',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True,\n",
    "                            save_weights_only=True,  # Save only the weights, not the full model\n",
    "                            mode='min')\n",
    "\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                    mode=\"min\", \n",
    "                    patience=5)\n",
    "callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class AdamAccumulate(tf.keras.optimizers.Optimizer):\n",
    "    def __init__(self, learning_rate=0.001, accum_iters=4, **kwargs):\n",
    "        super(AdamAccumulate, self).__init__(name=\"AdamAccumulate\", **kwargs)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.accum_iters = tf.constant(accum_iters, dtype=tf.float32)\n",
    "        self.iterations_accum = tf.Variable(0, dtype=tf.int64)\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        \"\"\" Create slots for all trainable variables \"\"\"\n",
    "        for var in var_list:\n",
    "            if self.has_slot(var, 'accum_grad'):\n",
    "                continue\n",
    "            self.add_slot(var, 'accum_grad')\n",
    "    \n",
    "    def apply_gradients(self, grads_and_vars, name=None, experimental_aggregate_gradients=True):\n",
    "        \"\"\" Apply accumulated gradients after accum_iters steps \"\"\"\n",
    "        grads, variables = zip(*grads_and_vars)\n",
    "        self._create_slots(variables)  # Ensure slots are created for all variables\n",
    "\n",
    "        accum_grads = [self.get_slot(var, 'accum_grad') for var in variables]\n",
    "        self.iterations_accum.assign_add(1)\n",
    "\n",
    "        # Accumulate the gradients\n",
    "        for g, accum_grad in zip(grads, accum_grads):\n",
    "            accum_grad.assign_add(g)\n",
    "\n",
    "        def apply_accumulated_grads():\n",
    "            applied_grads = []\n",
    "            for accum_grad, var in zip(accum_grads, variables):\n",
    "                applied_grads.append(self._apply_gradients(var, accum_grad / self.accum_iters))\n",
    "                accum_grad.assign(tf.zeros_like(accum_grad))  # Reset the gradients\n",
    "            return tf.group(*applied_grads)\n",
    "\n",
    "        # Apply accumulated gradients only every accum_iters steps\n",
    "        apply_grads_op = tf.cond(\n",
    "            tf.equal(self.iterations_accum % self.accum_iters, 0),\n",
    "            true_fn=apply_accumulated_grads,\n",
    "            false_fn=lambda: tf.no_op()\n",
    "        )\n",
    "        return apply_grads_op\n",
    "\n",
    "    def _apply_gradients(self, var, grad):\n",
    "        \"\"\" Update the weights based on the accumulated gradients \"\"\"\n",
    "        return var.assign_sub(self.learning_rate * grad)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\" Return the optimizer config for serialization \"\"\"\n",
    "        config = {'learning_rate': self.learning_rate, 'accum_iters': self.accum_iters.numpy()}\n",
    "        base_config = super(AdamAccumulate, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "\n",
    "class AccumModel(tf.keras.Model):\n",
    "    def __init__(self, num_classes, IMG_SIZE, channels=1, accum_steps=4):\n",
    "        super(AccumModel, self).__init__()\n",
    "        self.base_model = MobileNet(input_shape=(*IMG_SIZE, channels),\n",
    "                                    include_top=False, weights=None)\n",
    "        self.pooling = GlobalAveragePooling2D()\n",
    "        self.dropout1 = Dropout(0.5)\n",
    "        self.dense1 = Dense(512, activation='relu')\n",
    "        self.dropout2 = Dropout(0.5)\n",
    "        self.dense2 = Dense(num_classes, activation='sigmoid')\n",
    "        self.accum_steps = accum_steps  # Number of steps to accumulate gradients\n",
    "        self.step = 0\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(AccumModel, self).build(input_shape)\n",
    "        # Initialize accum_grads after the model is built as tf.Variables\n",
    "        self.accum_grads = [tf.Variable(tf.zeros_like(var), trainable=False) for var in self.trainable_variables]\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.base_model(inputs, training=training)\n",
    "        x = self.pooling(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        return self.dense2(x)\n",
    "\n",
    "    def reset_accumulated_gradients(self):\n",
    "        for var in self.accum_grads:\n",
    "            var.assign(tf.zeros_like(var))\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "\n",
    "        # Accumulate gradients\n",
    "        for i, grad in enumerate(gradients):\n",
    "            self.accum_grads[i].assign_add(grad)\n",
    "\n",
    "        self.step += 1\n",
    "\n",
    "        if self.step % self.accum_steps == 0:\n",
    "            # Apply accumulated gradients\n",
    "            self.optimizer.apply_gradients(zip(self.accum_grads, self.trainable_variables))\n",
    "            # Reset accumulated gradients\n",
    "            self.reset_accumulated_gradients()\n",
    "\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_SIZE_LIST = [(1024, 1024), (512, 512), (256, 256), (224, 224), (192, 192), (128, 128), (64, 64)]\n",
    "BATCH_SIZE_LIST = [4, 8, 16, 32, 32, 32, 64]\n",
    "\n",
    "STEPS_PER_EPOCH = 10000\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeModel(IMG_SIZE, num_classes, channels=1, accum_steps=4):\n",
    "    model = AccumModel(num_classes, IMG_SIZE, channels, accum_steps)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28235 validated image filenames belonging to 13 classes.\n",
      "Found 9414 validated image filenames belonging to 13 classes.\n",
      "Running Image Size:  (1024, 1024) Running Batch size :  4 Learning Rate :  0.0005\n",
      "Epoch 1/10\n",
      " 7059/10000 [====================>.........] - ETA: 16:14 - loss: 0.7656 - binary_accuracy: 0.5068 - mae: 0.4949WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 100000 batches). You may need to use the repeat() function when building your dataset.\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.71697, saving model to xray_class_weights.best.hdf5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to synchronously create dataset (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[182], line 53\u001b[0m\n\u001b[0;32m     49\u001b[0m opt \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m     50\u001b[0m multi_disease_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mopt, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     51\u001b[0m                             metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 53\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_disease_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEPS_PER_EPOCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     63\u001b[0m p \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Application Installed\\anacondaa\\envs\\sheakh310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\Application Installed\\anacondaa\\envs\\sheakh310\\lib\\site-packages\\h5py\\_hl\\group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[1;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmake_new_dset(group, shape, dtype, data, name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[1;32md:\\Application Installed\\anacondaa\\envs\\sheakh310\\lib\\site-packages\\h5py\\_hl\\dataset.py:163\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[1;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     sid \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(shape, maxshape)\n\u001b[1;32m--> 163\u001b[0m dset_id \u001b[38;5;241m=\u001b[39m \u001b[43mh5d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[0;32m    166\u001b[0m     dset_id\u001b[38;5;241m.\u001b[39mwrite(h5s\u001b[38;5;241m.\u001b[39mALL, h5s\u001b[38;5;241m.\u001b[39mALL, data)\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5d.pyx:137\u001b[0m, in \u001b[0;36mh5py.h5d.create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to synchronously create dataset (name already exists)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "run_this_code = True  # This Code Block Takes a long time to run\n",
    "\n",
    "if run_this_code:\n",
    "    train_results = defaultdict(dict)\n",
    "    test_results = defaultdict(dict)\n",
    "\n",
    "    lr = 0.0005\n",
    "    syntheticBatch = 256\n",
    "\n",
    "    plt.figure(figsize=(20, 5))\n",
    "\n",
    "    for imageSize, batchSize in zip(IMG_SIZE_LIST, BATCH_SIZE_LIST):  # Number of batches before gradient accumulation\n",
    "\n",
    "        batch = int(256 / batchSize)  # Gradient Accumulation Step\n",
    "\n",
    "        train_gen = core_idg.flow_from_dataframe(dataframe=train_df,\n",
    "                                                directory=None,\n",
    "                                                x_col='path',\n",
    "                                                y_col='newLabel',\n",
    "                                                class_mode='categorical',\n",
    "                                                classes=all_labels,\n",
    "                                                target_size=imageSize,\n",
    "                                                color_mode='grayscale',\n",
    "                                                batch_size=batchSize)\n",
    "\n",
    "        valid_gen = core_idg.flow_from_dataframe(dataframe=valid_df,\n",
    "                                                irectory=None,\n",
    "                                                x_col='path',\n",
    "                                                y_col='newLabel',\n",
    "                                                class_mode='categorical',\n",
    "                                                classes=all_labels,\n",
    "                                                target_size=imageSize,\n",
    "                                                color_mode='grayscale',\n",
    "                                                batch_size=batchSize)  # we can use much larger batches for evaluation\n",
    "\n",
    "        print('Running Image Size: ', imageSize, 'Running Batch size : ',\n",
    "            batchSize, 'Learning Rate : ', lr)\n",
    "\n",
    "        predictions_train = pd.DataFrame()\n",
    "        predictions_test = pd.DataFrame()\n",
    "\n",
    "        multi_disease_model = MakeModel(imageSize, len(all_labels), accum_steps=batch)\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        multi_disease_model.compile(optimizer=opt, loss='binary_crossentropy',\n",
    "                                    metrics=['binary_accuracy', 'mae'])\n",
    "\n",
    "        history = multi_disease_model.fit(train_gen,\n",
    "                                        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                                        validation_data=valid_gen,\n",
    "                                        epochs=EPOCHS,\n",
    "                                        callbacks=callbacks_list,\n",
    "                                        validation_steps=1000)\n",
    "\n",
    "\n",
    "        plt.plot(history.history['val_loss'])\n",
    "\n",
    "        p = history.history['val_loss'][0]\n",
    "\n",
    "        del multi_disease_model, history\n",
    "        gc.collect()\n",
    "        print('*' * 50)\n",
    "        print('')\n",
    "\n",
    "        test_results[imageSize[0]][lr] = p\n",
    "        imageSizeFile = pd.DataFrame(test_results)\n",
    "        imageSizeFile.to_csv(\"imageSize.csv\", index=True)\n",
    "\n",
    "    plt.legend([str(x[0]) for x in IMG_SIZE_LIST], loc='upper right')\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    plt.savefig('image_size_selection.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheakh310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
